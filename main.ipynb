{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0a6da0",
   "metadata": {},
   "source": [
    "# TP – API “Digital Social Score”\n",
    "\n",
    "De l’analyse de texte à l’infrastructure Cloud sécurisée, scalable \n",
    "et conform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e8f40",
   "metadata": {},
   "source": [
    "## Etape 1 : Exploration, analyse et anonymisation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18069d",
   "metadata": {},
   "source": [
    "###  Téléchargons le dataset Toxic Comment Classification Datase (Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"thesofakillers/jigsaw-toxic-comment-classification-challenge\", split=\"train\")\n",
    "\n",
    "# Afficher les premier lignes\n",
    "df = pd.DataFrame(dataset)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9a12c",
   "metadata": {},
   "source": [
    "### Utilisons spaCy pour anonymiser les données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ceae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\"\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger spaCy pour l'anglais (une seule fois)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Patterns regex pour détecter les données personnelles\n",
    "EMAIL_PATTERN = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "PHONE_PATTERN = re.compile(r'\\b(?:\\+?1[-.\\s]?)?\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}\\b')\n",
    "\n",
    "def detect_personal_data(text):\n",
    "    \"\"\"\n",
    "    Détecte toutes les données personnelles pour la conformité RGPD\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return {\n",
    "            'has_personal_data': False,\n",
    "            'names': [],\n",
    "            'emails': [],\n",
    "            'phones': [],\n",
    "            'addresses': []\n",
    "        }\n",
    "    \n",
    "    text_str = str(text)\n",
    "    \n",
    "    # 1. EMAILS\n",
    "    emails = EMAIL_PATTERN.findall(text_str)\n",
    "    \n",
    "    # 2. TÉLÉPHONES  \n",
    "    phones = PHONE_PATTERN.findall(text_str)\n",
    "    \n",
    "    # 3. NOMS avec spaCy NER\n",
    "    doc = nlp(text_str)\n",
    "    names = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            names.append(ent.text)\n",
    "    \n",
    "    return {\n",
    "        'has_personal_data': bool(names or emails or phones),\n",
    "        'names': names,\n",
    "        'emails': emails,\n",
    "        'phones': phones,\n",
    "    }\n",
    "\n",
    "def anonymize_text(text, detection_result):\n",
    "    \"\"\"\n",
    "    Anonymise automatiquement le texte pour la conformité RGPD\n",
    "    \"\"\"\n",
    "    if not detection_result['has_personal_data']:\n",
    "        return text\n",
    "    \n",
    "    anonymized_text = str(text)\n",
    "    \n",
    "    for email in detection_result['emails']:\n",
    "        anonymized_text = anonymized_text.replace(email, \"[EMAIL]\")\n",
    "    \n",
    "    for phone in detection_result['phones']:\n",
    "        anonymized_text = anonymized_text.replace(phone, \"[PHONE]\")\n",
    "    \n",
    "    for name in detection_result['names']:\n",
    "        anonymized_text = anonymized_text.replace(name, \"[NAME]\")\n",
    "    \n",
    "    return anonymized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRÉATION DU DATASET ANONYMISÉ (df_anonymized)\n",
    "print(\"ANONYMISATION DU DATASET\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Travaillons avec 10 000 données\n",
    "max_to_anonymize = 10000\n",
    "\n",
    "# Taille réelle à traiter\n",
    "actual_size = min(max_to_anonymize, len(df))\n",
    "\n",
    "print(f\"PARAMETRES D'ANONYMISATION:\")\n",
    "print(f\"Dataset total: {len(df):,} commentaires\")\n",
    "print(f\"A anonymiser: {actual_size:,} commentaires\")\n",
    "\n",
    "# Créer une copie du DataFrame pour l'anonymisation\n",
    "df_anonymized = df.copy()\n",
    "\n",
    "# Statistiques de traitement\n",
    "comments_with_personal_data = 0\n",
    "total_anonymizations = 0\n",
    "\n",
    "print(f\"\\nAnonymisation en cours...\")\n",
    "\n",
    "# Traiter les commentaires avec barre de progression\n",
    "for i in tqdm(range(actual_size), desc=\"Anonymisation\"):\n",
    "    original_comment = df['comment_text'].iloc[i]\n",
    "    \n",
    "    # Détecter les données personnelles\n",
    "    detection = detect_personal_data(original_comment)\n",
    "    \n",
    "    # Si des données personnelles sont trouvées, anonymiser\n",
    "    if detection['has_personal_data']:\n",
    "        comments_with_personal_data += 1\n",
    "        \n",
    "        # Compter le nombre total d'éléments à anonymiser\n",
    "        num_elements = (len(detection['names']) + len(detection['emails']) + len(detection['phones']))\n",
    "        total_anonymizations += num_elements\n",
    "        \n",
    "        # Anonymiser le texte\n",
    "        anonymized_comment = anonymize_text(original_comment, detection)\n",
    "        \n",
    "        # Remplacer dans le DataFrame anonymisé\n",
    "        df_anonymized.loc[i, 'comment_text'] = anonymized_comment\n",
    "\n",
    "print(f\"Total commentaires traités: {actual_size:,}\")\n",
    "print(f\"Commentaires anonymisés: {comments_with_personal_data:,}\")\n",
    "print(f\"Pourcentage de commentaires affectés: {comments_with_personal_data/actual_size*100:.2f}%\")\n",
    "print(f\"Total d'éléments anonymisés: {total_anonymizations:,}\")\n",
    "\n",
    "# Sauvegarder le dataset anonymisé\n",
    "anonymized_filename = \"dataset_anonymized_rgpd_10k.csv\"\n",
    "df_anonymized.to_csv(anonymized_filename, index=False)\n",
    "print(f\"\\nDataset anonymisé sauvegardé: {anonymized_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a60d1",
   "metadata": {},
   "source": [
    "### Comparons la version initiale et anonymisée et justifions chaque choix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea0b92",
   "metadata": {},
   "source": [
    "## Etape 1 :  Préparation et entraînement d’un modèle IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5cc830",
   "metadata": {},
   "source": [
    "### Nettoyage des textes (ponctuation, emojis, casse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fonction complète de nettoyage de texte pour le NLP\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convertir en string au cas où\n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. CASSE : Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. EMOJIS : Supprimer les emojis (patterns Unicode)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                              u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                              u\"\\U0001F300-\\U0001F5FF\"  # symboles & pictogrammes\n",
    "                              u\"\\U0001F680-\\U0001F6FF\"  # transport & cartes\n",
    "                              u\"\\U0001F1E0-\\U0001F1FF\"  # drapeaux\n",
    "                              u\"\\U00002500-\\U00002BEF\"  # caractères chinois\n",
    "                              u\"\\U00002702-\\U000027B0\"\n",
    "                              u\"\\U00002702-\\U000027B0\"\n",
    "                              u\"\\U000024C2-\\U0001F251\"\n",
    "                              u\"\\U0001f926-\\U0001f937\"\n",
    "                              u\"\\U00010000-\\U0010ffff\"\n",
    "                              u\"\\u2640-\\u2642\"\n",
    "                              u\"\\u2600-\\u2B55\"\n",
    "                              u\"\\u200d\"\n",
    "                              u\"\\u23cf\"\n",
    "                              u\"\\u23e9\"\n",
    "                              u\"\\u231a\"\n",
    "                              u\"\\ufe0f\"  # dingbats\n",
    "                              u\"\\u3030\"\n",
    "                              \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # 3. CARACTÈRES SPÉCIAUX : Supprimer les caractères non-ASCII\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # 4. URLs : Supprimer les URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
    "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
    "    \n",
    "    # 5. MENTIONS : Supprimer les @mentions\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    \n",
    "    # 6. HASHTAGS : Supprimer les #hashtags\n",
    "    text = re.sub(r'#\\w+', ' ', text)\n",
    "    \n",
    "    # 7. CHIFFRES : Supprimer les chiffres isolés\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "    \n",
    "    # 8. PONCTUATION : Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 9. ESPACES : Normaliser les espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 10. TRIM : Supprimer les espaces en début/fin\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e763131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÉTAPE 3: Application du nettoyage au dataset anonymisé\n",
    "print(\"NETTOYAGE DU DATASET ANONYMISE\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Vérifier que df_anonymized existe\n",
    "if 'df_anonymized' not in locals():\n",
    "    print(\"df_anonymized n'existe pas encore!\")\n",
    "else:\n",
    "    # Travailler seulement sur les 10 000 données anonymisées\n",
    "    max_to_clean = 10000\n",
    "    actual_size_to_clean = min(max_to_clean, len(df_anonymized))\n",
    "    \n",
    "    # Créer une copie du dataset anonymisé pour le nettoyage (seulement les 10k)\n",
    "    df_cleaned = df_anonymized.head(actual_size_to_clean).copy()\n",
    "    \n",
    "    # Statistiques de nettoyage\n",
    "    total_comments = len(df_cleaned)\n",
    "    original_total_words = 0\n",
    "    cleaned_total_words = 0\n",
    "    empty_after_cleaning = 0\n",
    "    \n",
    "    print(f\"Nettoyage de {total_comments:,} commentaires anonymisés...\")\n",
    "    \n",
    "    # Appliquer le nettoyage avec barre de progression\n",
    "    for i in tqdm(range(total_comments), desc=\"Nettoyage des commentaires\"):\n",
    "        original_comment = df_cleaned['comment_text'].iloc[i]\n",
    "        \n",
    "        if original_comment and not pd.isna(original_comment):\n",
    "            # Compter les mots avant nettoyage\n",
    "            original_total_words += len(str(original_comment).split())\n",
    "            \n",
    "            # Appliquer le nettoyage standard\n",
    "            cleaned_comment = clean_text(original_comment)\n",
    "            \n",
    "            # Remplacer dans le DataFrame\n",
    "            df_cleaned.loc[i, 'comment_text'] = cleaned_comment\n",
    "            \n",
    "            # Compter les mots après nettoyage\n",
    "            if cleaned_comment:\n",
    "                cleaned_total_words += len(cleaned_comment.split())\n",
    "            else:\n",
    "                empty_after_cleaning += 1\n",
    "    \n",
    "\n",
    "    print(f\"Total commentaires traités: {total_comments:,}\")\n",
    "    if original_total_words > 0:\n",
    "        print(f\"Réduction: {((original_total_words - cleaned_total_words) / original_total_words * 100):.1f}%\")\n",
    "    print(f\"Commentaires vides après nettoyage: {empty_after_cleaning}\")\n",
    "    \n",
    "    # Supprimer les commentaires vides\n",
    "    df_cleaned = df_cleaned[df_cleaned['comment_text'].str.len() > 0]\n",
    "    print(f\"Commentaires conservés: {len(df_cleaned):,}\")\n",
    "    \n",
    "    # Sauvegarder le dataset nettoyé\n",
    "    cleaned_filename = \"dataset_cleaned_and_anonymized_10k.csv\"\n",
    "    df_cleaned.to_csv(cleaned_filename, index=False)\n",
    "    print(f\"\\nDataset nettoyé sauvegardé: {cleaned_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cdf91a",
   "metadata": {},
   "source": [
    "### Entraînons deux modèles : statistique(SVM) et avancé (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARATION DU DATASET FINAL POUR L'ENTRAINEMENT\n",
    "\n",
    "# Vérifier que df_cleaned existe\n",
    "if 'df_cleaned' not in locals():\n",
    "    print(\"ERREUR: df_cleaned n'existe pas!\")\n",
    "    print(\"Veuillez d'abord exécuter les étapes précédentes.\")\n",
    "else:\n",
    "    # Sélectionner uniquement les colonnes nécessaires\n",
    "    required_columns = ['comment_text', 'toxic']\n",
    "    \n",
    "    # Vérifier que les colonnes existent\n",
    "    available_columns = list(df_cleaned.columns)\n",
    "    missing_columns = [col for col in required_columns if col not in available_columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"ATTENTION: Colonnes manquantes: {missing_columns}\")\n",
    "        # Utiliser les colonnes disponibles\n",
    "        final_columns = [col for col in required_columns if col in available_columns]\n",
    "    else:\n",
    "        final_columns = required_columns\n",
    "    \n",
    "    print(f\"Colonnes finales utilisées: {final_columns}\")\n",
    "    \n",
    "    # Créer le dataset final\n",
    "    df_final = df_cleaned[final_columns].copy()\n",
    "    \n",
    " \n",
    "    \n",
    "    # Distribution du label TOXIC\n",
    "    if 'toxic' in df_final.columns:\n",
    "        toxic_dist = df_final['toxic'].value_counts()\n",
    "        print(f\"\\nDISTRIBUTION DU LABEL TOXIC:\")\n",
    "        print(f\"Non-toxic (0): {toxic_dist.get(0, 0):,} ({toxic_dist.get(0, 0)/len(df_final)*100:.1f}%)\")\n",
    "        print(f\"Toxic (1): {toxic_dist.get(1, 0):,} ({toxic_dist.get(1, 0)/len(df_final)*100:.1f}%)\")\n",
    "        \n",
    "        # Vérifier l'équilibre des classes\n",
    "        toxic_ratio = toxic_dist.get(1, 0) / len(df_final) * 100\n",
    "        if toxic_ratio < 5:\n",
    "            print(f\"ATTENTION: Classes très déséquilibrées ({toxic_ratio:.1f}% toxic)\")\n",
    "        elif toxic_ratio < 20:\n",
    "            print(f\" Classes modérément déséquilibrées ({toxic_ratio:.1f}% toxic)\")\n",
    "        else:\n",
    "            print(f\" Classes relativement équilibrées ({toxic_ratio:.1f}% toxic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8442af9",
   "metadata": {},
   "source": [
    "Les models sont dans le dossier model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd39c7e",
   "metadata": {},
   "source": [
    "### Comparons précision, rappel, et temps de traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df94db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "\n",
    "class ModelBenchmark:\n",
    "    def __init__(self, test_data_path=\"test_toxic_10k.csv\"):\n",
    "        \"\"\"Initialiser le benchmark avec les données de test\"\"\"\n",
    "        print(\" BENCHMARK DES MODÈLES - TOXIC DETECTION\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Charger les données de test\n",
    "        self.test_df = pd.read_csv(test_data_path)\n",
    "        self.X_test = self.test_df['comment_text'].tolist()\n",
    "        self.y_test = self.test_df['toxic'].tolist()\n",
    "        \n",
    "        print(f\"Dataset de test: {len(self.X_test)} commentaires\")\n",
    "        print(f\"Distribution: {np.bincount(self.y_test)} [Non-toxic, Toxic]\")\n",
    "        print()\n",
    "        \n",
    "        self.results = {}\n",
    "    \n",
    "    def benchmark_svm(self):\n",
    "        \"\"\"Benchmarker le modèle SVM\"\"\"\n",
    "        print(\" BENCHMARK SVM\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        try:\n",
    "            # Charger le modèle SVM sauvegardé\n",
    "            svm_path = \"model/svm_pipeline.pkl\"\n",
    "            if not os.path.exists(svm_path):\n",
    "                print(f\" Modèle SVM non trouvé: {svm_path}\")\n",
    "                return None\n",
    "            \n",
    "            with open(svm_path, 'rb') as f:\n",
    "                svm_model = pickle.load(f)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Prédictions\n",
    "            predictions = svm_model.predict(self.X_test)\n",
    "            \n",
    "            # Essayer d'obtenir les probabilités\n",
    "            probabilities = None\n",
    "            try:\n",
    "                # SVM n'a pas predict_proba par défaut, utiliser decision_function\n",
    "                decision_scores = svm_model.decision_function(self.X_test)\n",
    "                # Convertir en probabilités approximatives avec sigmoid\n",
    "                probabilities = 1 / (1 + np.exp(-decision_scores))\n",
    "            except:\n",
    "                probabilities = None\n",
    "            \n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Calculer les métriques\n",
    "            accuracy = accuracy_score(self.y_test, predictions)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                self.y_test, predictions, average='binary'\n",
    "            )\n",
    "            \n",
    "            # AUC si probabilities disponibles\n",
    "            auc = None\n",
    "            if probabilities is not None:\n",
    "                try:\n",
    "                    auc = roc_auc_score(self.y_test, probabilities)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Matrice de confusion\n",
    "            cm = confusion_matrix(self.y_test, predictions)\n",
    "            \n",
    "            svm_results = {\n",
    "                'model_name': 'SVM',\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'auc': auc,\n",
    "                'inference_time': inference_time,\n",
    "                'inference_per_sample': inference_time / len(self.X_test),\n",
    "                'confusion_matrix': cm.tolist(),\n",
    "                'predictions': predictions\n",
    "            }\n",
    "            \n",
    "            print(f\"SVM - Temps d'inférence: {inference_time:.2f}s\")\n",
    "            print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"   F1-Score: {f1:.4f}\")\n",
    "            print()\n",
    "            \n",
    "            self.results['SVM'] = svm_results\n",
    "            return svm_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Erreur SVM: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def benchmark_bert(self):\n",
    "        \"\"\"Benchmarker le modèle BERT\"\"\"\n",
    "        print(\" BENCHMARK BERT\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        try:\n",
    "            # Charger le modèle BERT sauvegardé\n",
    "            bert_path = \"./bert_model\"\n",
    "            if not os.path.exists(bert_path):\n",
    "                print(f\" Modèle BERT non trouvé: {bert_path}\")\n",
    "                return None\n",
    "            \n",
    "            # Charger tokenizer et modèle\n",
    "            tokenizer = AutoTokenizer.from_pretrained(bert_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(bert_path)\n",
    "            model.eval()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Tokenisation\n",
    "            encoded = tokenizer(\n",
    "                self.X_test, \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=128, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Prédictions\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**encoded)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                predictions = torch.argmax(logits, dim=-1).numpy()\n",
    "                proba_toxic = probabilities[:, 1].numpy()  # Probabilité classe toxic (1)\n",
    "            \n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Calculer les métriques\n",
    "            accuracy = accuracy_score(self.y_test, predictions)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                self.y_test, predictions, average='binary'\n",
    "            )\n",
    "            \n",
    "            # AUC\n",
    "            auc = roc_auc_score(self.y_test, proba_toxic)\n",
    "            \n",
    "            # Matrice de confusion\n",
    "            cm = confusion_matrix(self.y_test, predictions)\n",
    "            \n",
    "            bert_results = {\n",
    "                'model_name': 'BERT',\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'auc': auc,\n",
    "                'inference_time': inference_time,\n",
    "                'inference_per_sample': inference_time / len(self.X_test),\n",
    "                'confusion_matrix': cm.tolist(),\n",
    "                'predictions': predictions\n",
    "            }\n",
    "            \n",
    "            print(f\" BERT - Temps d'inférence: {inference_time:.2f}s\")\n",
    "            print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"   F1-Score: {f1:.4f}\")\n",
    "            print()\n",
    "            \n",
    "            self.results['BERT'] = bert_results\n",
    "            return bert_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur BERT: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Comparer les résultats des deux modèles\"\"\"\n",
    "        if len(self.results) < 2:\n",
    "            print(\"Pas assez de modèles pour la comparaison\")\n",
    "            return\n",
    "        \n",
    "        print(\" COMPARAISON DES MODÈLES\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        svm_res = self.results.get('SVM', {})\n",
    "        bert_res = self.results.get('BERT', {})\n",
    "        \n",
    "        # Tableau de comparaison\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc', 'inference_time']\n",
    "        \n",
    "        print(f\"{'Métrique':<15} {'SVM':<10} {'BERT':<10} {'Gagnant':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        svm_wins = 0\n",
    "        bert_wins = 0\n",
    "        \n",
    "        for metric in metrics:\n",
    "            svm_val = svm_res.get(metric, 0)\n",
    "            bert_val = bert_res.get(metric, 0)\n",
    "            \n",
    "            if svm_val is None: svm_val = 0\n",
    "            if bert_val is None: bert_val = 0\n",
    "            \n",
    "            # Pour le temps d'inférence, plus petit = mieux\n",
    "            if metric == 'inference_time':\n",
    "                winner = \"SVM\" if svm_val < bert_val and svm_val > 0 else \"BERT\"\n",
    "                if winner == \"SVM\": svm_wins += 1\n",
    "                else: bert_wins += 1\n",
    "            else:\n",
    "                winner = \"SVM\" if svm_val > bert_val else \"BERT\"\n",
    "                if winner == \"SVM\": svm_wins += 1\n",
    "                else: bert_wins += 1\n",
    "            \n",
    "            print(f\"{metric:<15} {svm_val:<10.4f} {bert_val:<10.4f} {winner:<10}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Victoires SVM: {svm_wins}\")\n",
    "        print(f\"Victoires BERT: {bert_wins}\")\n",
    "        \n",
    "        # Recommandation\n",
    "        overall_winner = \"SVM\" if svm_wins > bert_wins else \"BERT\"\n",
    "        print(f\"\\n MODÈLE RECOMMANDÉ: {overall_winner}\")\n",
    "        \n",
    "        # Analyse détaillée\n",
    "        print(f\"\\n ANALYSE DÉTAILLÉE:\")\n",
    "        \n",
    "        if overall_winner == \"SVM\":\n",
    "            print(\" Avantages SVM:\")\n",
    "            print(\"   - Plus rapide en inférence\")\n",
    "            print(\"   - Moins de ressources nécessaires\")\n",
    "            print(\"   - Plus simple à déployer\")\n",
    "        else:\n",
    "            print(\" Avantages BERT:\")\n",
    "            print(\"   - Meilleure compréhension contextuelle\")\n",
    "            print(\"   - Performance généralement supérieure\")\n",
    "            print(\"   - Transfert learning efficace\")\n",
    "        \n",
    "        return overall_winner\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"Sauvegarder les résultats\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"benchmark_results_{timestamp}.json\"\n",
    "        \n",
    "        # Préparer les données pour JSON\n",
    "        json_results = {}\n",
    "        for model_name, results in self.results.items():\n",
    "            json_results[model_name] = {}\n",
    "            for key, value in results.items():\n",
    "                if key == 'predictions':\n",
    "                    continue  # Skip predictions (trop volumineux)\n",
    "                elif isinstance(value, np.ndarray):\n",
    "                    json_results[model_name][key] = value.tolist()\n",
    "                elif isinstance(value, np.floating):\n",
    "                    json_results[model_name][key] = float(value)\n",
    "                else:\n",
    "                    json_results[model_name][key] = value\n",
    "        \n",
    "        # Ajouter métadonnées\n",
    "        json_results['metadata'] = {\n",
    "            'timestamp': timestamp,\n",
    "            'test_samples': len(self.X_test),\n",
    "            'toxic_ratio': sum(self.y_test) / len(self.y_test),\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n Résultats sauvegardés: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def run_full_benchmark(self):\n",
    "        \"\"\"Exécuter le benchmark complet\"\"\"\n",
    "        # Benchmarker SVM\n",
    "        self.benchmark_svm()\n",
    "        \n",
    "        # Benchmarker BERT\n",
    "        self.benchmark_bert()\n",
    "        \n",
    "        # Comparer\n",
    "        winner = self.compare_models()\n",
    "        \n",
    "        # Sauvegarder\n",
    "        self.save_results()\n",
    "        \n",
    "        return winner\n",
    "\n",
    "# Lancer le benchmark\n",
    "benchmark = ModelBenchmark()\n",
    "winner = benchmark.run_full_benchmark()\n",
    "    \n",
    "print(f\"\\n CONCLUSION:\")\n",
    "print(f\"Le modèle recommandé est: {winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07880867",
   "metadata": {},
   "source": [
    "## Étape 3 : Déploiement du modèle en API Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570245a",
   "metadata": {},
   "source": [
    "### Exportons le modèle et le déployons dans le Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc238196",
   "metadata": {},
   "source": [
    "### Création d'une API Fast API recevant un texte et renvoyant un score."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
