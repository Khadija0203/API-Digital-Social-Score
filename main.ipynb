{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0a6da0",
   "metadata": {},
   "source": [
    "# TP ‚Äì API ‚ÄúDigital Social Score‚Äù\n",
    "\n",
    "De l‚Äôanalyse de texte √† l‚Äôinfrastructure Cloud s√©curis√©e, scalable \n",
    "et conform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e8f40",
   "metadata": {},
   "source": [
    "## Etape 1 : Exploration, analyse et anonymisation des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18069d",
   "metadata": {},
   "source": [
    "###  T√©l√©chargons le dataset **Toxic Comment Classification Datase**(Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1c6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xadee\\OneDrive\\Bureau\\API\\API-Digital-Social-Score\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"thesofakillers/jigsaw-toxic-comment-classification-challenge\", split=\"train\")\n",
    "\n",
    "# Afficher les premier lignes\n",
    "df = pd.DataFrame(dataset)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c37e1",
   "metadata": {},
   "source": [
    "### Analyse de la structure du dataset\n",
    "\n",
    "Analysons la structure, les dimensions et la distribution des labels du dataset Toxic Comment Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DIMENSIONS\n",
    "print(f\"\\nüìè DIMENSIONS:\")\n",
    "print(f\"   Nombre de lignes: {len(df):,}\")\n",
    "print(f\"   Nombre de colonnes: {len(df.columns)}\")\n",
    "print(f\"   Colonnes: {list(df.columns)}\")\n",
    "\n",
    "# 2. TYPES DE DONN√âES\n",
    "print(f\"\\nüìã TYPES DE DONN√âES:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# 3. VALEURS MANQUANTES\n",
    "print(f\"\\nüîç VALEURS MANQUANTES:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"   ‚úÖ Aucune valeur manquante\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "# 4. DISTRIBUTION DES LABELS\n",
    "print(f\"\\nüìä DISTRIBUTION DES LABELS (% de commentaires toxiques):\")\n",
    "label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for col in label_columns:\n",
    "    count = df[col].sum()\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   {col:15s}: {count:6,} ({percentage:5.2f}%)\")\n",
    "\n",
    "# 5. STATISTIQUES SUR LA LONGUEUR DES TEXTES\n",
    "df['text_length'] = df['comment_text'].astype(str).apply(len)\n",
    "df['word_count'] = df['comment_text'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "print(f\"\\nüìù STATISTIQUES SUR LES TEXTES:\")\n",
    "print(f\"   Longueur moyenne: {df['text_length'].mean():.0f} caract√®res\")\n",
    "print(f\"   Longueur m√©diane: {df['text_length'].median():.0f} caract√®res\")\n",
    "print(f\"   Nombre de mots moyen: {df['word_count'].mean():.0f} mots\")\n",
    "\n",
    "# 6. EXEMPLES DE COMMENTAIRES\n",
    "print(f\"\\nüí¨ EXEMPLES DE COMMENTAIRES:\")\n",
    "print(f\"\\n   Non-toxique:\")\n",
    "non_toxic = df[df['toxic'] == 0]['comment_text'].iloc[0]\n",
    "print(f\"   {non_toxic[:150]}...\")\n",
    "\n",
    "print(f\"\\n   Toxique:\")\n",
    "toxic = df[df['toxic'] == 1]['comment_text'].iloc[0]\n",
    "print(f\"   {toxic[:150]}...\")\n",
    "\n",
    "# 7. JUSTIFICATION DU CHOIX DE 10K √âCHANTILLONS\n",
    "print(f\"\\nüéØ JUSTIFICATION DU CHOIX DE 10,000 √âCHANTILLONS:\")\n",
    "print(f\"   ‚úì Dataset complet: {len(df):,} commentaires (tr√®s volumineux)\")\n",
    "print(f\"   ‚úì √âchantillon de 10k: repr√©sentatif et g√©rable\")\n",
    "print(f\"   ‚úì Permet un entra√Ænement rapide pour le TP\")\n",
    "print(f\"   ‚úì Conservation de la distribution des classes\")\n",
    "print(f\"   ‚úì Conformit√© RGPD: limitation des donn√©es trait√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9a12c",
   "metadata": {},
   "source": [
    "### Utilisons spaCy pour anonymiser les donn√©es "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ceae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\"\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger spaCy pour l'anglais (une seule fois)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Patterns regex pour d√©tecter les donn√©es personnelles\n",
    "EMAIL_PATTERN = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "PHONE_PATTERN = re.compile(r'\\b(?:\\+?1[-.\\s]?)?\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}\\b')\n",
    "\n",
    "def detect_personal_data(text):\n",
    "    \"\"\"\n",
    "    D√©tecte toutes les donn√©es personnelles pour la conformit√© RGPD\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return {\n",
    "            'has_personal_data': False,\n",
    "            'names': [],\n",
    "            'emails': [],\n",
    "            'phones': [],\n",
    "            'addresses': []\n",
    "        }\n",
    "    \n",
    "    text_str = str(text)\n",
    "    \n",
    "    # 1. EMAILS\n",
    "    emails = EMAIL_PATTERN.findall(text_str)\n",
    "    \n",
    "    # 2. T√âL√âPHONES  \n",
    "    phones = PHONE_PATTERN.findall(text_str)\n",
    "    \n",
    "    # 3. NOMS avec spaCy NER\n",
    "    doc = nlp(text_str)\n",
    "    names = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            names.append(ent.text)\n",
    "    \n",
    "    return {\n",
    "        'has_personal_data': bool(names or emails or phones),\n",
    "        'names': names,\n",
    "        'emails': emails,\n",
    "        'phones': phones,\n",
    "    }\n",
    "\n",
    "def anonymize_text(text, detection_result):\n",
    "    \"\"\"\n",
    "    Anonymise automatiquement le texte pour la conformit√© RGPD\n",
    "    \"\"\"\n",
    "    if not detection_result['has_personal_data']:\n",
    "        return text\n",
    "    \n",
    "    anonymized_text = str(text)\n",
    "    \n",
    "    for email in detection_result['emails']:\n",
    "        anonymized_text = anonymized_text.replace(email, \"[EMAIL]\")\n",
    "    \n",
    "    for phone in detection_result['phones']:\n",
    "        anonymized_text = anonymized_text.replace(phone, \"[PHONE]\")\n",
    "    \n",
    "    for name in detection_result['names']:\n",
    "        anonymized_text = anonymized_text.replace(name, \"[NAME]\")\n",
    "    \n",
    "    return anonymized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CR√âATION DU DATASET ANONYMIS√â (df_anonymized)\n",
    "print(\"ANONYMISATION DU DATASET\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Travaillons avec 10 000 donn√©es\n",
    "max_to_anonymize = 10000\n",
    "\n",
    "# Taille r√©elle √† traiter\n",
    "actual_size = min(max_to_anonymize, len(df))\n",
    "\n",
    "print(f\"PARAMETRES D'ANONYMISATION:\")\n",
    "print(f\"Dataset total: {len(df):,} commentaires\")\n",
    "print(f\"A anonymiser: {actual_size:,} commentaires\")\n",
    "\n",
    "# Cr√©er une copie du DataFrame pour l'anonymisation\n",
    "df_anonymized = df.copy()\n",
    "\n",
    "# Statistiques de traitement\n",
    "comments_with_personal_data = 0\n",
    "total_anonymizations = 0\n",
    "\n",
    "print(f\"\\nAnonymisation en cours...\")\n",
    "\n",
    "# Traiter les commentaires avec barre de progression\n",
    "for i in tqdm(range(actual_size), desc=\"Anonymisation\"):\n",
    "    original_comment = df['comment_text'].iloc[i]\n",
    "    \n",
    "    # D√©tecter les donn√©es personnelles\n",
    "    detection = detect_personal_data(original_comment)\n",
    "    \n",
    "    # Si des donn√©es personnelles sont trouv√©es, anonymiser\n",
    "    if detection['has_personal_data']:\n",
    "        comments_with_personal_data += 1\n",
    "        \n",
    "        # Compter le nombre total d'√©l√©ments √† anonymiser\n",
    "        num_elements = (len(detection['names']) + len(detection['emails']) + len(detection['phones']))\n",
    "        total_anonymizations += num_elements\n",
    "        \n",
    "        # Anonymiser le texte\n",
    "        anonymized_comment = anonymize_text(original_comment, detection)\n",
    "        \n",
    "        # Remplacer dans le DataFrame anonymis√©\n",
    "        df_anonymized.loc[i, 'comment_text'] = anonymized_comment\n",
    "\n",
    "print(f\"Total commentaires trait√©s: {actual_size:,}\")\n",
    "print(f\"Commentaires anonymis√©s: {comments_with_personal_data:,}\")\n",
    "print(f\"Pourcentage de commentaires affect√©s: {comments_with_personal_data/actual_size*100:.2f}%\")\n",
    "print(f\"Total d'√©l√©ments anonymis√©s: {total_anonymizations:,}\")\n",
    "\n",
    "# Sauvegarder le dataset anonymis√©\n",
    "anonymized_filename = \"dataset_anonymized_rgpd_10k.csv\"\n",
    "df_anonymized.to_csv(anonymized_filename, index=False)\n",
    "print(f\"\\nDataset anonymis√© sauvegard√©: {anonymized_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f930edb",
   "metadata": {},
   "source": [
    "### Comparaison AVANT/APR√àS Anonymisation\n",
    "\n",
    "Visualisons des exemples concrets d'anonymisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver des commentaires avec donn√©es personnelles\n",
    "examples_to_show = []\n",
    "for i in range(min(10000, len(df))):\n",
    "    original = df['comment_text'].iloc[i]\n",
    "    detection = detect_personal_data(original)\n",
    "    \n",
    "    if detection['has_personal_data']:\n",
    "        anonymized = anonymize_text(original, detection)\n",
    "        examples_to_show.append({\n",
    "            'index': i,\n",
    "            'original': original,\n",
    "            'anonymized': anonymized,\n",
    "            'detection': detection\n",
    "        })\n",
    "        \n",
    "        if len(examples_to_show) >= 10:  # Afficher 10 exemples\n",
    "            break\n",
    "\n",
    "# Afficher les exemples\n",
    "for idx, example in enumerate(examples_to_show, 1):\n",
    "    print(f\"\\n EXEMPLE {idx}:\")\n",
    "    print(f\"   Index: {example['index']}\")\n",
    "    print(f\"\\n    AVANT (donn√©es personnelles d√©tect√©es):\")\n",
    "    print(f\"      {example['original'][:200]}...\")\n",
    "    print(f\"\\n   D√©tections:\")\n",
    "    if example['detection']['names']:\n",
    "        print(f\"      - Noms: {example['detection']['names']}\")\n",
    "    if example['detection']['emails']:\n",
    "        print(f\"      - Emails: {example['detection']['emails']}\")\n",
    "    if example['detection']['phones']:\n",
    "        print(f\"      - T√©l√©phones: {example['detection']['phones']}\")\n",
    "    print(f\"\\n    APR√àS (anonymis√©):\")\n",
    "    print(f\"      {example['anonymized'][:200]}...\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(f\"\\n Total d'exemples trouv√©s: {len(examples_to_show)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CR√âATION DU TABLEAU R√âCAPITULATIF\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "for example in examples_to_show[:5]:  # Top 5 pour le tableau\n",
    "    comparison_data.append({\n",
    "        'Index': example['index'],\n",
    "        'Noms': len(example['detection']['names']),\n",
    "        'Emails': len(example['detection']['emails']),\n",
    "        'T√©l√©phones': len(example['detection']['phones']),\n",
    "        'Total Anonymisations': (\n",
    "            len(example['detection']['names']) + \n",
    "            len(example['detection']['emails']) + \n",
    "            len(example['detection']['phones'])\n",
    "        )\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\nTABLEAU R√âCAPITULATIF DES ANONYMISATIONS:\")\n",
    "print(df_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b0b99",
   "metadata": {},
   "source": [
    "###  Justification des choix d'anonymisation\n",
    "\n",
    "**Pourquoi ces m√©thodes ?**\n",
    "\n",
    "1. **NER (Named Entity Recognition) avec spaCy** :\n",
    "   - D√©tection automatique des noms de personnes\n",
    "   - Pr√©cision √©lev√©e pour l'anglais\n",
    "   - Mod√®le pr√©-entra√Æn√© robuste\n",
    "\n",
    "2. **Regex pour emails et t√©l√©phones** :\n",
    "   - Patterns standards reconnus\n",
    "   - Compl√©mentaire au NER\n",
    "   - Haute sensibilit√©\n",
    "\n",
    "3. **Remplacement par tokens** :\n",
    "   - `[NAME]`, `[EMAIL]`, `[PHONE]` : tokens g√©n√©riques\n",
    "   - Pr√©serve la structure du texte\n",
    "   - Permet l'analyse sans donn√©es personnelles\n",
    "   - Anonymisation irr√©versible\n",
    "\n",
    "**Conformit√© RGPD :**\n",
    "- Article 4(5) : Pseudonymisation effective\n",
    "- Article 25 : Privacy by design\n",
    "- Article 30 : Registre des traitements\n",
    "- Article 32 : S√©curit√© du traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a60d1",
   "metadata": {},
   "source": [
    "### Comparons la version initiale et anonymis√©e et justifions chaque choix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea0b92",
   "metadata": {},
   "source": [
    "## Etape 2 :  Pr√©paration et entra√Ænement d‚Äôun mod√®le IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5cc830",
   "metadata": {},
   "source": [
    "### Nettoyage des textes (ponctuation, emojis, casse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fonction compl√®te de nettoyage de texte pour le NLP\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convertir en string au cas o√π\n",
    "    text = str(text)\n",
    "    \n",
    "    # 1. CASSE : Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. EMOJIS : Supprimer les emojis (patterns Unicode)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                              u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                              u\"\\U0001F300-\\U0001F5FF\"  # symboles & pictogrammes\n",
    "                              u\"\\U0001F680-\\U0001F6FF\"  # transport & cartes\n",
    "                              u\"\\U0001F1E0-\\U0001F1FF\"  # drapeaux\n",
    "                              u\"\\U00002500-\\U00002BEF\"  # caract√®res chinois\n",
    "                              u\"\\U00002702-\\U000027B0\"\n",
    "                              u\"\\U00002702-\\U000027B0\"\n",
    "                              u\"\\U000024C2-\\U0001F251\"\n",
    "                              u\"\\U0001f926-\\U0001f937\"\n",
    "                              u\"\\U00010000-\\U0010ffff\"\n",
    "                              u\"\\u2640-\\u2642\"\n",
    "                              u\"\\u2600-\\u2B55\"\n",
    "                              u\"\\u200d\"\n",
    "                              u\"\\u23cf\"\n",
    "                              u\"\\u23e9\"\n",
    "                              u\"\\u231a\"\n",
    "                              u\"\\ufe0f\"  # dingbats\n",
    "                              u\"\\u3030\"\n",
    "                              \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # 3. CARACT√àRES SP√âCIAUX : Supprimer les caract√®res non-ASCII\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # 4. URLs : Supprimer les URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
    "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
    "    \n",
    "    # 5. MENTIONS : Supprimer les @mentions\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    \n",
    "    # 6. HASHTAGS : Supprimer les #hashtags\n",
    "    text = re.sub(r'#\\w+', ' ', text)\n",
    "    \n",
    "    # 7. CHIFFRES : Supprimer les chiffres isol√©s\n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
    "    \n",
    "    # 8. PONCTUATION : Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 9. ESPACES : Normaliser les espaces multiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 10. TRIM : Supprimer les espaces en d√©but/fin\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e763131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âTAPE 3: Application du nettoyage au dataset anonymis√©\n",
    "print(\"NETTOYAGE DU DATASET ANONYMISE\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# V√©rifier que df_anonymized existe\n",
    "if 'df_anonymized' not in locals():\n",
    "    print(\"df_anonymized n'existe pas encore!\")\n",
    "else:\n",
    "    # Travailler seulement sur les 10 000 donn√©es anonymis√©es\n",
    "    max_to_clean = 10000\n",
    "    actual_size_to_clean = min(max_to_clean, len(df_anonymized))\n",
    "    \n",
    "    # Cr√©er une copie du dataset anonymis√© pour le nettoyage (seulement les 10k)\n",
    "    df_cleaned = df_anonymized.head(actual_size_to_clean).copy()\n",
    "    \n",
    "    # Statistiques de nettoyage\n",
    "    total_comments = len(df_cleaned)\n",
    "    original_total_words = 0\n",
    "    cleaned_total_words = 0\n",
    "    empty_after_cleaning = 0\n",
    "    \n",
    "    print(f\"Nettoyage de {total_comments:,} commentaires anonymis√©s...\")\n",
    "    \n",
    "    # Appliquer le nettoyage avec barre de progression\n",
    "    for i in tqdm(range(total_comments), desc=\"Nettoyage des commentaires\"):\n",
    "        original_comment = df_cleaned['comment_text'].iloc[i]\n",
    "        \n",
    "        if original_comment and not pd.isna(original_comment):\n",
    "            # Compter les mots avant nettoyage\n",
    "            original_total_words += len(str(original_comment).split())\n",
    "            \n",
    "            # Appliquer le nettoyage standard\n",
    "            cleaned_comment = clean_text(original_comment)\n",
    "            \n",
    "            # Remplacer dans le DataFrame\n",
    "            df_cleaned.loc[i, 'comment_text'] = cleaned_comment\n",
    "            \n",
    "            # Compter les mots apr√®s nettoyage\n",
    "            if cleaned_comment:\n",
    "                cleaned_total_words += len(cleaned_comment.split())\n",
    "            else:\n",
    "                empty_after_cleaning += 1\n",
    "    \n",
    "\n",
    "    print(f\"Total commentaires trait√©s: {total_comments:,}\")\n",
    "    if original_total_words > 0:\n",
    "        print(f\"R√©duction: {((original_total_words - cleaned_total_words) / original_total_words * 100):.1f}%\")\n",
    "    print(f\"Commentaires vides apr√®s nettoyage: {empty_after_cleaning}\")\n",
    "    \n",
    "    # Supprimer les commentaires vides\n",
    "    df_cleaned = df_cleaned[df_cleaned['comment_text'].str.len() > 0]\n",
    "    print(f\"Commentaires conserv√©s: {len(df_cleaned):,}\")\n",
    "    \n",
    "    # Sauvegarder le dataset nettoy√©\n",
    "    cleaned_filename = \"dataset_cleaned_and_anonymized_10k.csv\"\n",
    "    df_cleaned.to_csv(cleaned_filename, index=False)\n",
    "    print(f\"\\nDataset nettoy√© sauvegard√©: {cleaned_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cdf91a",
   "metadata": {},
   "source": [
    "### Entra√Ænons le mod√®le‚ÄØavec SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier que df_cleaned existe\n",
    "if 'df_cleaned' not in locals():\n",
    "    print(\"ERREUR: df_cleaned n'existe pas!\")\n",
    "    print(\"Veuillez d'abord ex√©cuter les √©tapes pr√©c√©dentes.\")\n",
    "else:\n",
    "    # S√©lectionner uniquement les colonnes n√©cessaires\n",
    "    required_columns = ['comment_text', 'toxic']\n",
    "    \n",
    "    # V√©rifier que les colonnes existent\n",
    "    available_columns = list(df_cleaned.columns)\n",
    "    missing_columns = [col for col in required_columns if col not in available_columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"ATTENTION: Colonnes manquantes: {missing_columns}\")\n",
    "        # Utiliser les colonnes disponibles\n",
    "        final_columns = [col for col in required_columns if col in available_columns]\n",
    "    else:\n",
    "        final_columns = required_columns\n",
    "    \n",
    "    print(f\"Colonnes finales utilis√©es: {final_columns}\")\n",
    "    \n",
    "    # Cr√©er le dataset final\n",
    "    df_final = df_cleaned[final_columns].copy()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29d3e7",
   "metadata": {},
   "source": [
    "### Split Train/Test\n",
    "\n",
    "Divisons le dataset en ensembles d'entra√Ænement et de test tout en pr√©servant la distribution des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# V√©rifier que df_final existe\n",
    "if 'df_final' not in locals():\n",
    "    print(\" ERREUR: df_final n'existe pas!\")\n",
    "    print(\"Veuillez ex√©cuter les cellules pr√©c√©dentes.\")\n",
    "else:\n",
    "    # Extraire X (textes) et y (labels)\n",
    "    X = df_final['comment_text'].values\n",
    "    y = df_final['toxic'].values\n",
    "    \n",
    "    # Split stratifi√© 80/20\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2,     \n",
    "        random_state=42, \n",
    "        stratify=y          # Pr√©server la distribution des classes\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n R√âSULTATS DU SPLIT:\")\n",
    "    print(f\"   Dataset total: {len(df_final):,} commentaires\")\n",
    "    print(f\"   Train: {len(X_train):,} commentaires ({len(X_train)/len(df_final)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {len(X_test):,} commentaires ({len(X_test)/len(df_final)*100:.1f}%)\")\n",
    "    \n",
    "    # Sauvegarder les datasets\n",
    "    train_df = pd.DataFrame({'comment_text': X_train, 'toxic': y_train})\n",
    "    test_df = pd.DataFrame({'comment_text': X_test, 'toxic': y_test})\n",
    "    \n",
    "    train_filename = \"data/train_toxic_10k.csv\"\n",
    "    test_filename = \"data/test_toxic_10k.csv\"\n",
    "    \n",
    "    train_df.to_csv(train_filename, index=False)\n",
    "    test_df.to_csv(test_filename, index=False)\n",
    "    \n",
    "    print(f\"\\n FICHIERS SAUVEGARD√âS:\")\n",
    "    print(f\"   Train: {train_filename}\")\n",
    "    print(f\"   Test:  {test_filename}\")\n",
    "    print(\"\\n Split termin√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06f2d9",
   "metadata": {},
   "source": [
    "### Entra√Ænement du Mod√®le SVM\n",
    "\n",
    "Lan√ßons l'entra√Ænement du mod√®le SVM avec le pipeline TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# V√©rifier que le fichier d'entra√Ænement existe\n",
    "train_file = \"data/train_toxic_10k.csv\"\n",
    "if not os.path.exists(train_file):\n",
    "    print(f\" ERREUR: {train_file} n'existe pas!\")\n",
    "    print(\"Veuillez ex√©cuter la cellule pr√©c√©dente (Split Train/Test).\")\n",
    "else:\n",
    "    print(f\" Fichier d'entra√Ænement trouv√©: {train_file}\")\n",
    "    print(f\"\\n Lancement de l'entra√Ænement SVM...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Ex√©cuter le script SVM.py\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"model/SVM.py\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300  # Timeout de 5 minutes\n",
    "        )\n",
    "        \n",
    "        # Afficher la sortie\n",
    "        if result.stdout:\n",
    "            print(result.stdout)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"\\n ENTRA√éNEMENT SVM TERMIN√â AVEC SUCC√àS!\")\n",
    "            \n",
    "            # V√©rifier que le mod√®le a √©t√© sauvegard√©\n",
    "            model_path = \"model/svm_pipeline.pkl\"\n",
    "            if os.path.exists(model_path):\n",
    "                model_size = os.path.getsize(model_path) / (1024 * 1024)  # MB\n",
    "                print(f\" Mod√®le sauvegard√©: {model_path} ({model_size:.2f} MB)\")\n",
    "            else:\n",
    "                print(f\"  Attention: {model_path} non trouv√©\")\n",
    "        else:\n",
    "            print(\"\\n ERREUR lors de l'entra√Ænement:\")\n",
    "            if result.stderr:\n",
    "                print(result.stderr)\n",
    "                \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\" TIMEOUT: L'entra√Ænement a pris trop de temps (>5 min)\")\n",
    "    except Exception as e:\n",
    "        print(f\" ERREUR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d03333",
   "metadata": {},
   "source": [
    "###  √âvaluation du mod√®le SVM\n",
    "\n",
    "√âvaluons les performances du mod√®le SVM sur l'ensemble de test avec m√©triques d√©taill√©es et visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# √âVALUATION DU MOD√àLE SVM\n",
    "print(\"=\" * 60)\n",
    "print(\" √âVALUATION DU MOD√àLE SVM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Charger le mod√®le\n",
    "model_path = \"model/svm_pipeline.pkl\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\" ERREUR: Mod√®le non trouv√©: {model_path}\")\n",
    "    print(\"Veuillez d'abord entra√Æner le mod√®le (cellule pr√©c√©dente).\")\n",
    "else:\n",
    "    # Charger le mod√®le\n",
    "    with open(model_path, 'rb') as f:\n",
    "        svm_model = pickle.load(f)\n",
    "    print(f\" Mod√®le charg√©: {model_path}\")\n",
    "    \n",
    "    # Charger les donn√©es de test\n",
    "    test_df = pd.read_csv(\"data/test_toxic_10k.csv\")\n",
    "    X_test = test_df['comment_text'].values\n",
    "    y_test = test_df['toxic'].values\n",
    "    \n",
    "    print(f\" Donn√©es de test charg√©es: {len(X_test)} commentaires\")\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    print(f\"\\n Pr√©dictions en cours...\")\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    # Calculer les m√©triques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    \n",
    "    print(f\"\\n M√âTRIQUES DE PERFORMANCE:\")\n",
    "    print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall:    {recall:.4f}\")\n",
    "    print(f\"   F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Classification report d√©taill√©\n",
    "    print(f\"\\n CLASSIFICATION REPORT D√âTAILL√â:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Non-toxic', 'Toxic']))\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n MATRICE DE CONFUSION:\")\n",
    "    print(f\"                  Pr√©dit Non-toxic  Pr√©dit Toxic\")\n",
    "    print(f\"R√©el Non-toxic         {cm[0][0]:6d}        {cm[0][1]:6d}\")\n",
    "    print(f\"R√©el Toxic             {cm[1][0]:6d}        {cm[1][1]:6d}\")\n",
    "    \n",
    "    # Visualiser la matrice de confusion\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-toxic', 'Toxic'],\n",
    "                yticklabels=['Non-toxic', 'Toxic'])\n",
    "    plt.title('Matrice de Confusion - SVM')\n",
    "    plt.ylabel('Vraie Classe')\n",
    "    plt.xlabel('Classe Pr√©dite')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('svm_confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n Matrice de confusion sauvegard√©e: svm_confusion_matrix.png\")\n",
    "    print(f\"\\n √âVALUATION TERMIN√âE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49475446",
   "metadata": {},
   "source": [
    "###  Tests de Pr√©diction - Exemples Concrets\n",
    "\n",
    "Testons le mod√®le SVM sur des exemples concrets pour v√©rifier son comportement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le mod√®le si n√©cessaire\n",
    "if 'svm_model' not in locals():\n",
    "    with open(\"model/svm_pipeline.pkl\", 'rb') as f:\n",
    "        svm_model = pickle.load(f)\n",
    "\n",
    "# Exemples de test\n",
    "test_examples = [\n",
    "    # Non-toxiques\n",
    "    \"This is a great article, thank you for sharing!\",\n",
    "    \"I respectfully disagree with your point of view.\",\n",
    "    \"Can you please explain this concept in more detail?\",\n",
    "    \n",
    "    # Toxiques\n",
    "    \"You are stupid and your opinion is worthless!\",\n",
    "    \"I hate you and everything you stand for.\",\n",
    "    \"Shut up, nobody cares about what you think!\",\n",
    "    \n",
    "    # Cas limites\n",
    "    \"This is ridiculous.\",\n",
    "    \"I don't like this approach.\",\n",
    "    \"You're wrong about this.\",\n",
    "]\n",
    "\n",
    "print(f\"\\n Test sur {len(test_examples)} exemples:\\n\")\n",
    "\n",
    "for i, text in enumerate(test_examples, 1):\n",
    "    # Pr√©diction\n",
    "    prediction = svm_model.predict([text])[0]\n",
    "    label = \" TOXIC\" if prediction == 1 else \" NON-TOXIC\"\n",
    "    \n",
    "    # Obtenir le score de d√©cision (confiance)\n",
    "    try:\n",
    "        decision_score = svm_model.decision_function([text])[0]\n",
    "        confidence = abs(decision_score)\n",
    "    except:\n",
    "        confidence = 0\n",
    "    \n",
    "    print(f\"{i:2d}. {label} (confiance: {confidence:.2f})\")\n",
    "    print(f\"    Texte: \\\"{text}\\\"\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07880867",
   "metadata": {},
   "source": [
    "## √âtape 3 : D√©ploiement du mod√®le en API Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc238196",
   "metadata": {},
   "source": [
    "### Cr√©ation d'une API Fast API recevant un texte et renvoyant un score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7649263c",
   "metadata": {},
   "source": [
    "###  Architecture de l'API FastAPI\n",
    "\n",
    "Notre API de d√©tection de toxicit√© est construite avec **FastAPI** et d√©ploy√©e sur **Google Kubernetes Engine (GKE)**.\n",
    "\n",
    "**Endpoints principaux :**\n",
    "\n",
    "1. **`POST /token`** : Authentification JWT\n",
    "   - Entr√©e : `username` et `password`\n",
    "   - Sortie : Token JWT valide 24h\n",
    "\n",
    "2. **`POST /predict`** : Pr√©diction de toxicit√©\n",
    "   - Entr√©e : `text` (commentaire √† analyser)\n",
    "   - Sortie : `score` (0-100) et `is_toxic` (bool√©en)\n",
    "   - Authentification : JWT requis\n",
    "\n",
    "3. **`GET /health`** : V√©rification de sant√©\n",
    "   - Sortie : √âtat de l'API et du mod√®le\n",
    "\n",
    "4. **`GET /metrics`** : M√©triques Prometheus\n",
    "   - Sortie : M√©triques pour supervision\n",
    "\n",
    "**S√©curit√© :**\n",
    "- Authentification JWT (python-jose)\n",
    "- Validation des entr√©es (pydantic)\n",
    "- Rate limiting\n",
    "- CORS configur√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d6e263",
   "metadata": {},
   "source": [
    "### Test de l'API en Local\n",
    "\n",
    "Testons l'API en local (localhost:8080) pour v√©rifier son fonctionnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e03e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# CONFIGURATION\n",
    "API_URL = \"http://localhost:8080\"  # API locale\n",
    "USERNAME = \"admin\"  \n",
    "PASSWORD = \"admin\"\n",
    "\n",
    "# √âTAPE 1: Obtenir un token JWT\n",
    "print(\"\\n √âTAPE 1: Authentification JWT\")\n",
    "\n",
    "try:\n",
    "    token_response = requests.post(\n",
    "        f\"{API_URL}/token\",\n",
    "        data={\n",
    "            \"username\": USERNAME,\n",
    "            \"password\": PASSWORD\n",
    "        },\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if token_response.status_code == 200:\n",
    "        token_data = token_response.json()\n",
    "        access_token = token_data['access_token']\n",
    "        print(f\" Token obtenu avec succ√®s!\")\n",
    "        print(f\"   Token type: {token_data['token_type']}\")\n",
    "        print(f\"   Token: {access_token[:50]}...\")\n",
    "    else:\n",
    "        print(f\" Erreur d'authentification: {token_response.status_code}\")\n",
    "        print(f\"   R√©ponse: {token_response.text}\")\n",
    "        access_token = None\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(f\" ERREUR: Impossible de se connecter √† {API_URL}\")\n",
    "    print(f\"   Assurez-vous que l'API est d√©marr√©e (python app.py)\")\n",
    "    access_token = None\n",
    "except Exception as e:\n",
    "    print(f\" ERREUR: {e}\")\n",
    "    access_token = None\n",
    "\n",
    "# √âTAPE 2: Tester les pr√©dictions\n",
    "if access_token:\n",
    "    print(f\"\\n √âTAPE 2: Tests de Pr√©diction\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Exemples de tests\n",
    "    test_cases = [\n",
    "        {\"text\": \"This is a great article, thank you!\", \"expected\": \"Non-toxic\"},\n",
    "        {\"text\": \"You are stupid and worthless!\", \"expected\": \"Toxic\"},\n",
    "        {\"text\": \"I disagree with your opinion.\", \"expected\": \"Non-toxic\"},\n",
    "    ]\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{API_URL}/predict\",\n",
    "                headers=headers,\n",
    "                json={\"text\": test[\"text\"]},\n",
    "                timeout=10\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                prediction = \" Toxic\" if result['is_toxic'] else \" Non-toxic\"\n",
    "                print(f\"\\nTest {i}: {prediction} (Score: {result['score']}/100)\")\n",
    "                print(f\"   Texte: \\\"{test['text']}\\\"\")\n",
    "                print(f\"   Attendu: {test['expected']}\")\n",
    "            else:\n",
    "                print(f\"\\n Erreur Test {i}: {response.status_code}\")\n",
    "                print(f\"   {response.text}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n Erreur Test {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6430b",
   "metadata": {},
   "source": [
    "### Test de l'API avec GKE\n",
    "\n",
    "Testons l'API d√©ploy√©e sur Google Kubernetes Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# CONFIGURATION PRODUCTION\n",
    "PROD_API_URL = \"http://34.22.130.34/docs\"  # IP publique GKE\n",
    "USERNAME = \"admin\"\n",
    "PASSWORD = \"admin\"\n",
    "\n",
    "# Test de sant√©\n",
    "print(f\"\\n Health check de l'API\")\n",
    "\n",
    "\n",
    "try:\n",
    "    health_response = requests.get(f\"{PROD_API_URL}/health\", timeout=10)\n",
    "    \n",
    "    if health_response.status_code == 200:\n",
    "        health_data = health_response.json()\n",
    "        print(f\" API op√©rationnelle!\")\n",
    "        print(f\"   Status: {health_data.get('status', 'N/A')}\")\n",
    "        print(f\"   Model: {health_data.get('model', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"  R√©ponse inattendue: {health_response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(f\" ERREUR: Impossible de se connecter √† {PROD_API_URL}\")\n",
    "    print(f\"   L'API de production est peut-√™tre arr√™t√©e.\")\n",
    "except Exception as e:\n",
    "    print(f\" ERREUR: {e}\")\n",
    "\n",
    "# Authentification\n",
    "print(f\"\\n Authentification JWT\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "try:\n",
    "    # Note: L'endpoint /token n'est peut-√™tre pas d√©ploy√© en production\n",
    "    token_response = requests.post(\n",
    "        f\"{PROD_API_URL}/token\",\n",
    "        data={\"username\": USERNAME, \"password\": PASSWORD},\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if token_response.status_code == 200:\n",
    "        token_data = token_response.json()\n",
    "        access_token = token_data['access_token']\n",
    "        print(f\" Token obtenu!\")\n",
    "        \n",
    "        # Test de pr√©diction\n",
    "        print(f\"\\n Test de Pr√©diction\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "        test_text = \"This is a test comment.\"\n",
    "        \n",
    "        pred_response = requests.post(\n",
    "            f\"{PROD_API_URL}/predict\",\n",
    "            headers=headers,\n",
    "            json={\"text\": test_text},\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if pred_response.status_code == 200:\n",
    "            result = pred_response.json()\n",
    "            print(f\" Pr√©diction r√©ussie!\")\n",
    "            print(f\"   Texte: \\\"{test_text}\\\"\")\n",
    "            print(f\"   Score: {result['score']}/100\")\n",
    "            print(f\"   Toxic: {result['is_toxic']}\")\n",
    "        else:\n",
    "            print(f\" Erreur de pr√©diction: {pred_response.status_code}\")\n",
    "            \n",
    "    elif token_response.status_code == 404:\n",
    "        print(f\"  Endpoint /token non trouv√© (404)\")\n",
    "        print(f\"   L'API de production doit √™tre red√©ploy√©e avec le endpoint /token\")\n",
    "    else:\n",
    "        print(f\" Erreur d'authentification: {token_response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(f\" Connexion impossible\")\n",
    "except Exception as e:\n",
    "    print(f\" ERREUR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6de59",
   "metadata": {},
   "source": [
    "## √âtape 4 : S√©curit√© et conformit√© RGPD\n",
    "\n",
    "### Mesures de S√©curit√© Impl√©ment√©es\n",
    "\n",
    "Notre API respecte les standards de s√©curit√© et la conformit√© RGPD :\n",
    "\n",
    "**1. Authentification JWT (JSON Web Tokens)**\n",
    "- Tokens sign√©s avec secret cryptographique\n",
    "- Expiration configurable (24h par d√©faut)\n",
    "- Validation syst√©matique des requ√™tes\n",
    "\n",
    "**2. Validation des entr√©es**\n",
    "- Pydantic models pour validation stricte\n",
    "- Sanitization des inputs\n",
    "- Protection contre injections\n",
    "\n",
    "**3. RGPD**\n",
    "- Anonymisation pr√©alable des donn√©es (spaCy NER)\n",
    "- Pas de stockage de donn√©es personnelles\n",
    "- Registre de traitement document√©\n",
    "\n",
    "**4. S√©curit√© Infrastructure**\n",
    "- Service Account GCP avec permissions minimales\n",
    "- Secrets dans Secret Manager (JWT_SECRET)\n",
    "- Network policies Kubernetes\n",
    "\n",
    "**5. Monitoring & Audit**\n",
    "- Logs centralis√©s (Cloud Logging)\n",
    "- M√©triques de s√©curit√© (tentatives auth √©chou√©es)\n",
    "- Alertes sur comportements anormaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcfe0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification des permissions IAM et Service Account\n",
    "print(\" Configuration IAM Kubernetes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "iam_config = {\n",
    "    \"Service Account\": \"mlops-api-sa\",\n",
    "    \"Namespace\": \"default\",\n",
    "    \"Permissions GCP\": [\n",
    "        \"storage.objects.get (Cloud Storage)\",\n",
    "        \"logging.logEntries.create (Cloud Logging)\",\n",
    "        \"monitoring.timeSeries.create (Cloud Monitoring)\",\n",
    "        \"aiplatform.endpoints.predict (Vertex AI)\"\n",
    "    ],\n",
    "    \"Principe\": \"Least Privilege (permissions minimales)\",\n",
    "    \"Workload Identity\": \"Recommand√© pour production\"\n",
    "}\n",
    "\n",
    "print(f\"Service Account Kubernetes: {iam_config['Service Account']}\")\n",
    "print(f\"\\nPermissions GCP attribu√©es :\")\n",
    "for perm in iam_config['Permissions']:\n",
    "    print(f\" {perm}\")\n",
    "\n",
    "print(f\"\\n Principe appliqu√©: {iam_config['Principe']}\")\n",
    "print(f\" {iam_config['Workload Identity']}\")\n",
    "\n",
    "print(\"\\n Commandes pour v√©rifier IAM :\")\n",
    "print(\"gcloud iam service-accounts list\")\n",
    "print(\"kubectl get serviceaccount mlops-api-sa -o yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53dea7",
   "metadata": {},
   "source": [
    "## √âtape 5 : Simulation de mont√©e en charge avec Locust\n",
    "\n",
    "### Objectifs des tests de charge\n",
    "\n",
    "Notre strat√©gie de load testing vise √† :\n",
    "- **Mesurer les performances** : Latence P50, P95, P99\n",
    "- **Identifier les limites** : Capacit√© maximale (RPS)\n",
    "- **D√©tecter les goulots** : CPU, m√©moire, I/O\n",
    "- **Valider la scalabilit√©** : Horizontal Pod Autoscaling\n",
    "\n",
    "### Configuration du test\n",
    "- **Outil** : Locust (Python)\n",
    "- **Sc√©narios** : Authentication + Pr√©dictions (mix toxique/non-toxique)\n",
    "- **Mont√©e en charge** : Progressive (spawn rate contr√¥l√©e)\n",
    "- **M√©triques cl√©s** : Response time, throughput, error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification du fichier locustfile.py\n",
    "import os\n",
    "\n",
    "locust_file = \"locustfile.py\"\n",
    "if os.path.exists(locust_file):\n",
    "    print(\" locustfile.py trouv√©\")\n",
    "    print(\"\\n Structure du test de charge :\")\n",
    "    \n",
    "    # Lecture des principales m√©triques\n",
    "    with open(locust_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    print(\"\\n Sc√©narios de test d√©tect√©s :\")\n",
    "    if \"def login\" in content:\n",
    "        print(\"  Authentication JWT (/token)\")\n",
    "    if \"def predict_toxicity\" in content:\n",
    "        print(\"  Pr√©dictions toxicit√© (/predict)\")\n",
    "    if \"def health_check\" in content:\n",
    "        print(\"  Health checks (/health)\")\n",
    "    print(\"  ‚Ä¢ Endpoint : http://localhost:8080 (ou production)\")\n",
    "else:\n",
    "    print(\" locustfile.py non trouv√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b402567",
   "metadata": {},
   "source": [
    "### Ex√©cution de la simulation de mont√©e en charge\n",
    "\n",
    "**Commande pour lancer Locust (mode headless) :**\n",
    "\n",
    "```bash\n",
    "# Test local (5 minutes, 50 utilisateurs max)\n",
    "locust -f locustfile.py --host=http://localhost:8080 --users 50 --spawn-rate 5 --run-time 5m --headless --html report_loadtest.html\n",
    "\n",
    "# Test production\n",
    "locust -f locustfile.py --host=http://34.77.253.251 --users 100 --spawn-rate 10 --run-time 10m --headless --html report_production.html\n",
    "```\n",
    "\n",
    "**Commande pour interface Web (mode interactif) :**\n",
    "\n",
    "```bash\n",
    "locust -f locustfile.py --host=http://localhost:8080\n",
    "# Ouvrir http://localhost:8089 dans le navigateur\n",
    "```\n",
    "\n",
    "**M√©triques :**\n",
    "- **P50 (M√©diane)** : Temps de r√©ponse pour 50% des requ√™tes\n",
    "- **P95** : 95% des requ√™tes r√©pondent sous ce temps (SLA typique)\n",
    "- **P99** : 99% des requ√™tes (d√©tection des outliers)\n",
    "- **RPS (Requests/sec)** : D√©bit maximal support√©\n",
    "- **Error Rate** : Taux d'erreur acceptable < 1%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b82966",
   "metadata": {},
   "source": [
    "## √âtape 6 : Monitoring avec Google Cloud Monitoring\n",
    "\n",
    "### Architecture de Monitoring\n",
    "\n",
    "Notre solution de monitoring utilise **Google Cloud Monitoring** pour :\n",
    "\n",
    "**1. M√©triques Kubernetes/GKE (automatiques)**\n",
    "- CPU/Memory des pods\n",
    "- Network I/O\n",
    "- Disk usage\n",
    "- Pod restarts, crashes\n",
    "\n",
    "**2. M√©triques Applicatives (custom)**\n",
    "- Request rate (requ√™tes/sec)\n",
    "- Latency (P50, P95, P99)\n",
    "- Error rate\n",
    "- Token generation rate\n",
    "- Model prediction count\n",
    "\n",
    "**3. Logs centralis√©s**\n",
    "- Cloud Logging (stdout/stderr des containers)\n",
    "- Filtres et recherche avanc√©e\n",
    "- Alertes bas√©es sur logs\n",
    "\n",
    "**4. Alerting**\n",
    "- Latency P95 > 500ms\n",
    "- Error rate > 5%\n",
    "- Pod availability < 2 replicas\n",
    "- Memory usage > 80%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
