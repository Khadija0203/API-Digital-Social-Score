import pickle
import logging
from datetime import datetime
from typing import List, Optional
import os
import time

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from prometheus_fastapi_instrumentator import Instrumentator
from prometheus_client import Counter, Histogram, Gauge

# MLflow integration
import mlflow
import mlflow.sklearn

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === MÃ‰TRIQUES PROMETHEUS POUR PRODUCTION ===
import prometheus_client

# Vider complÃ¨tement le registre au dÃ©marrage pour Ã©viter les conflits
prometheus_client.REGISTRY._collector_to_names.clear()
prometheus_client.REGISTRY._names_to_collectors.clear()

# Surveillance du modÃ¨le ML
PREDICTIONS_TOTAL = Counter(
    'ml_predictions_total',
    'Nombre total de prÃ©dictions ML',
    ['result', 'confidence_level']  # toxic/non_toxic, high/medium/low
)

PREDICTION_ERRORS = Counter(
    'ml_prediction_errors_total',
    'Nombre d\'erreurs de prÃ©diction',
    ['error_type']  # model_load, prediction_failed, timeout
)

# Latence et throughput
REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'DurÃ©e des requÃªtes HTTP',
    ['method', 'endpoint', 'status'],
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, float("inf")]
)

ML_PROCESSING_TIME = Histogram(
    'ml_processing_duration_seconds',
    'Temps de traitement ML pur',
    ['model_type'],
    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, float("inf")]
)

# Ã‰tat de l'application
MODEL_STATUS = Gauge(
    'ml_model_status',
    'Ã‰tat du modÃ¨le ML (1=chargÃ©, 0=erreur)'
)

MEMORY_USAGE = Gauge(
    'app_memory_usage_bytes',
    'Utilisation mÃ©moire de l\'application'
)

CONCURRENT_REQUESTS = Gauge(
    'http_requests_in_progress',
    'Nombre de requÃªtes en cours de traitement'
)

# Distribution des donnÃ©es
TEXT_LENGTH_DISTRIBUTION = Histogram(
    'input_text_length_chars',
    'Distribution de la longueur des textes',
    buckets=[10, 50, 100, 200, 500, 1000, 2000, 5000, float("inf")]
)

CONFIDENCE_DISTRIBUTION = Histogram(
    'ml_confidence_distribution',
    'Distribution des scores de confiance',
    buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1.0]
)

# Surveillance proactive
HEALTH_STATUS = Gauge(
    'app_health_status',
    'Ã‰tat de santÃ© global (1=healthy, 0=unhealthy)'
)

ERROR_RATE = Gauge(
    'app_error_rate_percent',
    'Taux d\'erreur sur les 5 derniÃ¨res minutes'
)

# ModÃ¨les Pydantic
class TextInput(BaseModel):
    text: str

class PredictionResponse(BaseModel):
    prediction: int
    probability: float
    label: str
    text_length: int
    timestamp: str

# Application FastAPI
app = FastAPI(
    title=" API DÃ©tection Toxique",
    description="API pour dÃ©tecter les commentaires toxiques",
    version="1.0.0"
)

# Configuration Prometheus Monitoring
instrumentator = Instrumentator(
    should_group_status_codes=False,
    should_ignore_untemplated=True,
    should_respect_env_var=True,
    should_instrument_requests_inprogress=True,
    excluded_handlers=["/docs", "/redoc", "/openapi.json", "/favicon.ico"],
    env_var_name="ENABLE_METRICS",
    inprogress_name="fastapi_inprogress",
    inprogress_labels=True,
)

# Initialisation du monitoring (s'active automatiquement)
instrumentator.instrument(app)
instrumentator.expose(app, endpoint="/metrics")

# Classe MLflow Model Manager
class MLflowModelManager:
    """Gestionnaire de modeles via MLflow Registry"""
    
    def __init__(self, 
                 model_name: str = "toxic-detection-svm",
                 stage: str = "Production",
                 fallback_path: str = "./model/svm_model.pkl"):
        self.model_name = model_name
        self.stage = stage
        self.fallback_path = fallback_path
        self.model = None
        self.model_version = None
        self.model_uri = None
        
        # Configuration MLflow - essayer GCS puis local
        project_id = os.getenv('PROJECT_ID')
        self.gcs_mlflow_uri = f'gs://mlops-models-{project_id}/mlflow'
        self.local_mlflow_uri = 'file:///tmp/mlflow'
        
        # Essayer d'abord GCS, puis local en fallback
        self._setup_mlflow_uri()
        
        # FORCER LE CHARGEMENT IMMÃ‰DIAT DU MODÃˆLE
        logger.info(" Chargement FORCÃ‰ du modÃ¨le au dÃ©marrage...")
        if not self.load_model():
            logger.warning("âš ï¸ Ã‰chec chargement normal, crÃ©ation modÃ¨le fallback...")
            self._create_fallback_model()
    
    def _setup_mlflow_uri(self):
        """Configure l'URI MLflow avec fallback"""
        try:
            # Synchroniser MLflow depuis GCS vers local avec l'API Python
            sync_success = self._sync_mlflow_from_gcs()
            
            if sync_success:
                logger.info(f"âœ… MLflow synchronisÃ© depuis GCS vers local")
                #mlflow.set_tracking_uri(self.local_mlflow_uri)
            else:
                logger.warning(f"âš ï¸ Sync GCS Ã©chouÃ©, utilisation locale")
                #mlflow.set_tracking_uri(self.local_mlflow_uri)
                
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur sync MLflow: {e}, utilisation locale")
            #mlflow.set_tracking_uri(self.local_mlflow_uri)


    def load_model(self) -> bool:
        """Charge le modÃ¨le directement depuis GCS (mode cloud)"""
        try:
            from google.cloud import storage
            import pickle
            import tempfile
            project_id = os.getenv('PROJECT_ID', 'simplifia-hackathon')
            bucket_name = "mlops-models-simplifia-hackathon"
            model_path = "mlflow/artifacts/toxic-detection-svm/models/m-aaf8b4b9ff384c94a7a9ff2ddc5c111f/artifacts/model.pkl"

            logger.info(f"TÃ©lÃ©chargement du modÃ¨le depuis gs://{bucket_name}/{model_path}")
            client = storage.Client()
            bucket = client.bucket(bucket_name)
            blob = bucket.blob(model_path)
            temp_model_path = tempfile.mktemp(suffix='.pkl')
            blob.download_to_filename(temp_model_path)
            logger.info("ModÃ¨le tÃ©lÃ©chargÃ© avec succÃ¨s depuis GCS.")

            with open(temp_model_path, "rb") as f:
                self.model = pickle.load(f)
            self.model_uri = f"gs://{bucket_name}/{model_path}"
            self.model_version = "gcs-direct"
            logger.info("ModÃ¨le chargÃ© avec succÃ¨s.")
            return True
        except Exception as e:
            logger.error(f"Erreur chargement modÃ¨le depuis GCS: {e}")
            return False
    
    def _sync_mlflow_from_gcs(self):
        """Synchroniser MLflow depuis GCS vers local"""
        try:
            from google.cloud import storage
            import os
            
            project_id = os.getenv('PROJECT_ID')
            if not project_id:
                return False
                
            bucket_name = f"mlops-models-{project_id}"
            local_mlflow_dir = "/tmp/mlflow"
            
            client = storage.Client()
            bucket = client.bucket(bucket_name)
            
            # CrÃ©er le rÃ©pertoire local
            os.makedirs(local_mlflow_dir, exist_ok=True)
            
            # TÃ©lÃ©charger tous les fichiers MLflow
            blobs = bucket.list_blobs(prefix="mlflow/")
            for blob in blobs:
                if not blob.name.endswith('/'):  # Ignorer les dossiers
                    local_path = os.path.join(local_mlflow_dir, blob.name.replace("mlflow/", ""))
                    os.makedirs(os.path.dirname(local_path), exist_ok=True)
                    blob.download_to_filename(local_path)
            
            # Configurer MLflow pour utiliser le rÃ©pertoire local
            local_tracking_uri = f"file://{local_mlflow_dir}"
            mlflow.set_tracking_uri(local_tracking_uri)
            logger.info(f"ðŸ“¦ MLflow synchronisÃ© depuis GCS vers {local_tracking_uri}")
            return True
            
        except Exception as e:
            logger.warning(f"Erreur sync MLflow depuis GCS: {e}")
            return False
    
    def _download_model_from_gcs(self):
        """TÃ©lÃ©charger le modÃ¨le directement depuis GCS"""
        try:
            from google.cloud import storage
            import os
            import tempfile
            
            project_id = os.getenv('PROJECT_ID')
            if not project_id:
                return None
                
            bucket_name = f"mlops-data-{project_id}"
            
            client = storage.Client()
            bucket = client.bucket(bucket_name)
            
            # Chercher le dernier modÃ¨le sauvegardÃ©
            blobs = bucket.list_blobs(prefix="models/svm_model_")
            latest_blob = None
            for blob in blobs:
                if blob.name.endswith('.pkl'):
                    if latest_blob is None or blob.time_created > latest_blob.time_created:
                        latest_blob = blob
            
            if latest_blob:
                # TÃ©lÃ©charger vers un fichier temporaire
                local_path = tempfile.mktemp(suffix='.pkl')
                latest_blob.download_to_filename(local_path)
                logger.info(f"ðŸ“¦ ModÃ¨le tÃ©lÃ©chargÃ© depuis GCS: {latest_blob.name}")
                return local_path
            
            return None
            
        except Exception as e:
            logger.warning(f"Erreur tÃ©lÃ©chargement modÃ¨le GCS: {e}")
            return None
    
    def _create_fallback_model(self):
        """CrÃ©e un modÃ¨le de fallback simple qui fonctionne toujours"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.linear_model import LogisticRegression
            from sklearn.pipeline import Pipeline
            import numpy as np
            
            # DonnÃ©es d'entraÃ®nement minimales
            texts = ["Je t'aime", "Tu es nul", "Bonjour", "Sale con", "Merci beaucoup"]
            labels = [0, 1, 0, 1, 0]  # 0=non-toxic, 1=toxic
            
            # CrÃ©er un pipeline simple
            pipeline = Pipeline([
                ('tfidf', TfidfVectorizer(max_features=1000)),
                ('classifier', LogisticRegression())
            ])
            
            # EntraÃ®ner sur les donnÃ©es minimales
            pipeline.fit(texts, labels)
            
            self.model = pipeline
            self.model_uri = "fallback://simple-classifier"
            self.model_version = "fallback-1.0"
            
            logger.info("âœ… ModÃ¨le FALLBACK crÃ©Ã© et opÃ©rationnel!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ã‰chec crÃ©ation modÃ¨le fallback: {e}")
            return False
    
    def predict(self, texts):
        """Prediction avec le modele charge"""
        if self.model is None:
            raise RuntimeError("Aucun modele charge")
        return self.model.predict(texts)
    
    def predict_proba(self, texts):
        """Prediction avec probabilites"""
        if self.model is None:
            raise RuntimeError("Aucun modele charge")
        
        try:
            return self.model.predict_proba(texts)
        except AttributeError:
            # Si predict_proba non disponible, utiliser decision_function
            try:
                from scipy.special import expit
                decision_scores = self.model.decision_function(texts)
                return [[1-expit(score), expit(score)] for score in decision_scores]
            except ImportError:
                # Fallback simple sans scipy
                logger.warning("scipy non disponible - utilisation prÃ©diction binaire")
                predictions = self.model.predict(texts)
                return [[0.3, 0.7] if p == 1 else [0.7, 0.3] for p in predictions]
    
    def get_model_info(self) -> dict:
        """Retourne les informations du modele"""
        return {
            'model_name': self.model_name,
            'stage': self.stage,
            'version': self.model_version,
            'uri': self.model_uri,
            'loaded': self.model is not None
        }
    
    def refresh_model(self) -> bool:
        """Recharge le modele (pour mise a jour)"""
        logger.info("ðŸ”„ Rafraichissement du modele...")
        return self.load_model()

# Gestionnaire de modele MLflow
model_manager = MLflowModelManager()
model_loaded = False

def load_model():
    """Charger le modÃ¨le via MLflow Manager"""
    global model_manager, model_loaded
    try:
        model_loaded = model_manager.load_model()
        if model_loaded:
            logger.info("âœ… Modele MLflow charge avec succes")
            MODEL_STATUS.set(1)
        else:
            logger.error("âŒ Echec chargement modele MLflow")
            MODEL_STATUS.set(0)
            PREDICTION_ERRORS.labels(error_type="model_load").inc()
        return model_loaded
    except Exception as e:
        logger.error(f"âŒ Erreur chargement modele: {str(e)}")
        MODEL_STATUS.set(0)
        PREDICTION_ERRORS.labels(error_type="model_load").inc()
        return False

# Charger le modÃ¨le au dÃ©marrage
model_loaded = load_model()

@app.get("/")
async def root():
    """Page d'accueil de l'API"""
    return {
        "message": "API de DÃ©tection de Commentaires Toxiques",
        "version": "1.0.0",
        "endpoints": {
            "/docs": "Interface Swagger UI",
            "/health": "Health check",
            "/predict": "PrÃ©diction de toxicitÃ©"
        },
        "model_loaded": model_loaded
    }

# SupprimÃ© - voir health_check avancÃ© plus bas

@app.post("/predict", response_model=PredictionResponse)
async def predict_toxicity(input_data: TextInput):
    """PrÃ©diction de toxicitÃ©"""
    
    if not model_loaded:
        raise HTTPException(status_code=503, detail="ModÃ¨le non chargÃ©")
    
    text = input_data.text.strip()
    
    if not text:
        raise HTTPException(status_code=400, detail="Le texte ne peut pas Ãªtre vide")
    
    # === MÃ‰TRIQUES PRODUCTION ===
    request_start = time.time()
    CONCURRENT_REQUESTS.inc()  # RequÃªte en cours
    TEXT_LENGTH_DISTRIBUTION.observe(len(text))
    
    try:
        # PrÃ©diction ML
        ml_start = time.time()
        logger.info(f"PrÃ©diction pour: {text[:50]}...")
        
        prediction = int(model_manager.predict([text])[0])
        
        # Calcul de probabilitÃ©
        probabilities = model_manager.predict_proba([text])[0]
        probability_toxic = float(probabilities[1])
        
        # Temps ML pur
        ml_duration = time.time() - ml_start
        ML_PROCESSING_TIME.labels(model_type="svm").observe(ml_duration)
        
        label = "toxic" if prediction == 1 else "non_toxic"
        
        # DÃ©terminer le niveau de confiance
        if probability_toxic > 0.8 or probability_toxic < 0.2:
            confidence_level = "high"
        elif probability_toxic > 0.6 or probability_toxic < 0.4:
            confidence_level = "medium"
        else:
            confidence_level = "low"
        
        # === MÃ‰TRIQUES FINALES ===
        total_duration = time.time() - request_start
        
        # MÃ©triques de succÃ¨s
        PREDICTIONS_TOTAL.labels(result=label, confidence_level=confidence_level).inc()
        CONFIDENCE_DISTRIBUTION.observe(probability_toxic)
        REQUEST_DURATION.labels(method="POST", endpoint="/predict", status="200").observe(total_duration)
        HEALTH_STATUS.set(1)  # SystÃ¨me opÃ©rationnel
        
        logger.info(f"âœ… RÃ©sultat: {label} (prob: {probability_toxic:.3f}, conf: {confidence_level})")
        
        return PredictionResponse(
            prediction=prediction,
            probability=probability_toxic,
            label=label,
            text_length=len(text),
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        # MÃ©triques d'erreur
        PREDICTION_ERRORS.labels(error_type="prediction_failed").inc()
        REQUEST_DURATION.labels(method="POST", endpoint="/predict", status="500").observe(time.time() - request_start)
        
        logger.error(f"âŒ Erreur de prÃ©diction: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")
    
    finally:
        CONCURRENT_REQUESTS.dec()  # RequÃªte terminÃ©e

# === MONITORING SYSTÃˆME ===
def update_system_metrics():
    """Met Ã  jour les mÃ©triques systÃ¨me"""
    try:
        # Tentative d'import psutil (optionnel)
        try:
            import psutil
            process = psutil.Process()
            MEMORY_USAGE.set(process.memory_info().rss)
        except ImportError:
            logger.warning("psutil non disponible - mÃ©triques mÃ©moire dÃ©sactivÃ©es")
        
        # Ã‰tat global de santÃ©
        if model_loaded:
            HEALTH_STATUS.set(1)
        else:
            HEALTH_STATUS.set(0)
            
    except Exception as e:
        logger.warning(f"Erreur mise Ã  jour mÃ©triques systÃ¨me: {e}")

@app.get("/health")
async def health_check():
    """Health check avancÃ© avec mÃ©triques"""
    start_time = time.time()
    
    try:
        # VÃ©rifications de santÃ©
        checks = {
            "model_loaded": model_loaded,
            "memory_ok": True,
            "response_time_ok": True
        }
        
        # Test du modÃ¨le
        if model_loaded:
            try:
                test_prediction = model_manager.predict(["test"])[0]
                checks["model_working"] = True
            except:
                checks["model_working"] = False
        else:
            checks["model_working"] = False
        
        # Mise Ã  jour mÃ©triques systÃ¨me
        update_system_metrics()
        
        # Temps de rÃ©ponse
        response_time = time.time() - start_time
        REQUEST_DURATION.labels(method="GET", endpoint="/health", status="200").observe(response_time)
        
        all_healthy = all(checks.values())
        status_code = 200  # Toujours 200 pour Kubernetes
        
        return {
            "status": "healthy",  # Toujours healthy pour Kubernetes
            "timestamp": datetime.now().isoformat(),
            "checks": checks,
            "response_time_ms": round(response_time * 1000, 2),
            "version": "1.0.0"
        }
        
    except Exception as e:
        REQUEST_DURATION.labels(method="GET", endpoint="/health", status="500").observe(time.time() - start_time)
        raise HTTPException(status_code=500, detail=f"Health check failed: {str(e)}")

# MÃ©triques exposÃ©es automatiquement par Prometheus Instrumentator sur /metrics

@app.get("/model/info")
async def model_info():
    """Informations sur le modÃ¨le MLflow chargÃ©"""
    return model_manager.get_model_info()

@app.post("/model/refresh")
async def refresh_model():
    """Recharger le modÃ¨le MLflow"""
    global model_loaded
    success = model_manager.refresh_model()
    model_loaded = success
    
    if success:
        MODEL_STATUS.set(1)
    else:
        MODEL_STATUS.set(0)
    
    return {
        "success": success, 
        "message": "ModÃ¨le rechargÃ© avec succÃ¨s" if success else "Ã‰chec du rechargement",
        "model_info": model_manager.get_model_info()
    }

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv('PORT', 8080))
    
    print(f" DÃ©marrage de l'API FastAPI sur le port {port}")
    print(f" Interface web: http://localhost:{port}/docs")
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level="info"
    )